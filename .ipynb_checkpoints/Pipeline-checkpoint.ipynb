{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291060b5-a88f-4d68-b6be-770789d7ee23",
   "metadata": {},
   "source": [
    "## Programa general\n",
    "\n",
    "En preparacion para hacer un despliegue en dash para hacer mas dinamico el analalis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e6fe4-7377-4338-81b7-8945c8362830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CargadorCSV:\n",
    "    \"\"\"Clase para carga de archivos con formato .csv\"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Constructor de la clase. Recibe la ruta del archivo.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def visualizar_avance(self, bytes_avance, bytes_completo):\n",
    "        \"\"\"Visualiza el avance de la carga del archivo en porcentaje.\"\"\"\n",
    "        avance = min(round(bytes_avance / bytes_completo, 2) * 100, 100)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Avance estimado: {avance}%\")\n",
    "    \n",
    "    def crear_csv_por_horarios(self, chunk_size=100000, sep=\"|\"):\n",
    "        \"\"\"Crea archivos CSV separados por horarios a partir del archivo original.\"\"\"\n",
    "        try:\n",
    "            bytes_completo = os.path.getsize(self.file_path)\n",
    "            encabezado = True\n",
    "            horarios = ['MEDIODIA', 'TARDE', 'NOCHE', 'MANANA']\n",
    "            \n",
    "            for chunk in pd.read_csv(self.file_path, sep=sep, chunksize=chunk_size):\n",
    "                bytes_avance = 0\n",
    "                \n",
    "                if encabezado:\n",
    "                    for clasificador_horario in horarios:\n",
    "                        df_filtered = chunk.loc[chunk[\"HORARIO\"] == clasificador_horario].drop(\"HORARIO\", axis=1)\n",
    "                        file_name = f\"{self.file_path[:-4]}_{clasificador_horario}.csv\"\n",
    "                        df_filtered.to_csv(file_name, index=False)\n",
    "                        bytes_avance += os.path.getsize(file_name)\n",
    "                    df_Nofiltered = chunk.loc[~chunk[\"HORARIO\"].isin(horarios)]\n",
    "                    df_Nofiltered.to_csv(f\"{self.file_path[:-4]}_ValoresPerdidos.csv \", index=False)\n",
    "                    \n",
    "            #df_MANANA_filtered = df_MANANA.loc[~df_MANANA[\"MES\"].isin(months_to_exclude)]\n",
    "                    encabezado = False\n",
    "                else:\n",
    "                    for clasificador_horario in horarios:\n",
    "                        df_filtered = chunk.loc[chunk[\"HORARIO\"] == clasificador_horario].drop(\"HORARIO\", axis=1)\n",
    "                        file_name = f\"{self.file_path[:-4]}_{clasificador_horario}.csv\"\n",
    "                        df_filtered.to_csv(file_name, mode='a', index=False, header=False)\n",
    "                        bytes_avance += os.path.getsize(file_name)\n",
    "                    df_Nofiltered = chunk.loc[~chunk[\"HORARIO\"].isin(horarios)]\n",
    "                    df_Nofiltered.to_csv(f\"{self.file_path[:-4]}_ValoresPerdidos.csv \",mode = 'a', index=False, header=False)                   \n",
    "                \n",
    "                self.visualizar_avance(bytes_avance, bytes_completo)\n",
    "            #clear_output(wait=True)\n",
    "            print(f\"Proceso terminado\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8dd901-7d2d-4521-9346-c24c6c6038e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga completa\n"
     ]
    }
   ],
   "source": [
    "#Resumen\n",
    "\n",
    "# Librerias a utilizar, por defecto ya vienen instaladas en el entorno \"pip install git jupyterlab numpy pandas plotly \"\n",
    "# para mejor visualizacion de los df y manejo de estadisticas:\n",
    "# ! pip install pandasgui\n",
    "# ! pip install pandas-profiling\n",
    "# ! pip install seaborn\n",
    "# ! pip install scipy\n",
    "#! pip install optbinning\n",
    "#! pip install ortools == 9.4.1874\n",
    "#! pip install -U scikit-learn\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasgui import show\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "import statsmodels.api as sm\n",
    "from optbinning import OptimalBinning\n",
    "from optbinning import BinningProcess\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "print(\"librerias cargadas correctamente\") \n",
    "\n",
    "class revisor_data_csv:\n",
    "    \"\"\"Clase para carga y revisión de archivos con formato .csv\n",
    "    Con esto podemos visualizar y manipular la data de forma controlada minimizando errores y\n",
    "    pudiendo posteriormente crar un pipeline del proceso ETL\"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Constructor de la clase que recibe la ruta del archivo y carga el archivo\n",
    "        Se asume archivos .csv estándar, sep = \",\"\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.mensaje(\"cargando\", self.file_path)\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            self.mensaje(\"cargado\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def mensaje(self, tipo, arg=None):\n",
    "        tipos_mensajes = {\"cargando\": f\"Cargando archivo {arg}\",\n",
    "                          \"cargado\": \"Carga completa\"}\n",
    "        if tipo in tipos_mensajes:\n",
    "            clear_output(wait=True)\n",
    "            print(tipos_mensajes[tipo])\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Tipo de mensaje no válido\")\n",
    "            print(f\"{list(tipos_mensajes.keys())}\")\n",
    "    \n",
    "    def analisis_nulos(self):\n",
    "        cuenta_nulos = self.df.isnull().sum()\n",
    "        cuenta_nulos = cuenta_nulos[cuenta_nulos != 0]\n",
    "        porcentaje_nulos = round((cuenta_nulos / self.df.shape[0]) * 100, 2)\n",
    "        return pd.DataFrame({\"cant_nulos\": cuenta_nulos, \"porcentaje_nulos\": porcentaje_nulos})\n",
    "\n",
    "    def nulos_EliminacionSimple(self, lista):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores nulos en las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.dropna(subset=lista)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def nulos_MediaCondicional(self, variable, lista):\n",
    "        \"\"\"Se asignará a los valores nulos de la columna variable,\n",
    "        el valor promedio agrupando con las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df.loc[self.df[variable].isnull(), variable] = self.df.groupby(lista, as_index=False)[[variable]].transform('mean')\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def eliminar_por_condicion(self,columna,condicion):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores 'condicion' en la columna indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.drop(self.df[self.df[columna] == condicion].index)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "#revision = revisor_data_csv(\"..\\\\datos_internos/20210513_Challenge_AA_MANANA.csv\")\n",
    "revision = revisor_data_csv(\"df_prueba_MANANA.csv\")\n",
    "\n",
    "# Eliminacion de valores nulos\n",
    "revision.nulos_EliminacionSimple(['MORAS', 'SEXO', 'ESTADOCIVIL', 'FECHANACIMIENTO', 'ANTIGUEDAD', 'EDAD'])\n",
    "revision.nulos_MediaCondicional('INGRESO',['ESTADO','EDAD','ESTADOCIVIL','SEXO'])\n",
    "revision.nulos_EliminacionSimple(['INGRESO'])\n",
    "revision.df['MORAS'] = revision.df['MORAS'].astype(int)\n",
    "revision.df['ANTIGUEDAD'] = revision.df['ANTIGUEDAD'].astype(int)\n",
    "revision.df['EDAD'] = revision.df['EDAD'].astype(int)\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO']).dt.date\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO'])\n",
    "revision.eliminar_por_condicion('ESTADOCIVIL',' ')\n",
    "\n",
    "revision.df = revision.df.drop(['CLIENTE'], axis=1)\n",
    "\n",
    "class preprocesamiento:\n",
    "    def __init__(self,revisor_data):\n",
    "        \"\"\"\n",
    "        Constructor de la clase recibe el objeto instanciado de la clase revisor_data_csv, \n",
    "        seria mejor usar un patron de diseño DataFrameSingleton pero esta division solo es \n",
    "        para fines demostrativos,en codigo final todos los metodos deben \n",
    "        pertenecer a una sola clase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.revisor_data = revisor_data\n",
    "            self.df = self.revisor_data.df\n",
    "            self.matriz_correlacion = None\n",
    "            self.transformaciones = {}\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    #Este metodo esta repetido en la clase graficos, al final se debe mejorar el codigo ya sea uniendo todas las clases o usando herencias.\n",
    "    def varibles_numericas(self):\n",
    "        \"\"\"Devuelve una lista con las variables numericas de un df\"\"\"\n",
    "        return [column_name for column_name, data_type in zip(self.df.columns, self.df.dtypes) if ((data_type != 'category') and  np.issubdtype(data_type, np.number))]\n",
    "    \n",
    "    def detecion_outlier(self,nombre_columna,q=.1):\n",
    "        \"\"\" \n",
    "        Funcion para detectar valores atípicos (utiliers) de una columna especifica usando cuantiles.\n",
    "        Por defecto se usa Deciles / dividiendo la distribucion en 10 partes, pero ajsutando el valor de\n",
    "          q = 0.25 se trabajaria con cuartiles.\n",
    "        La funcion devuelve una \"lista/pandas.core.indexes.numeric.Int64Index\"  con los indices de los \n",
    "        valores atípicos del df\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #calculo de cuantiles\n",
    "        Q1 = self.df[nombre_columna].quantile(q)\n",
    "        Q3 = self.df[nombre_columna].quantile(1-q)\n",
    "        IQR = Q3-Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        indice_filas_eliminar = self.df.index[(self.df[nombre_columna] < limite_inferior) | (self.df[nombre_columna] > limite_superior) ]\n",
    "        return indice_filas_eliminar   \n",
    "\n",
    "    def eliminacion_outlier(self,nombre_columna,q=0.1):\n",
    "        \"\"\" \n",
    "        Funcion para eliminar valores atípicos (outliers) de una columna especifica usando cuantiles.\n",
    "        La funcion no devuelve nada porque los cambios se hacen en el df que se pasa por referencia\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                indices = self.detecion_outlier(nombre_columna,q)\n",
    "                self.df = self.df.drop(indices)\n",
    "                self.df.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def guardar_transformador(self,nombre_columna,transformador):\n",
    "        \"\"\"Metodo para guardar transformador aplicado, se almacenan en lista, si se\n",
    "        aplican dos o mas se almacenaran en la secuencia aplicada\"\"\"\n",
    "        if nombre_columna in list(self.transformaciones.keys()):\n",
    "            self.transformaciones[nombre_columna] = self.transformaciones[nombre_columna]+[transformador]\n",
    "        else:\n",
    "            self.transformaciones[nombre_columna] = [transformador]\n",
    "            \n",
    "    def Transf_MinMaxScaler(self,nombre_columna):\n",
    "        \"\"\"crea objeto que Transforma los valores de la columna indicada \n",
    "        usando MinMaxScaler de sklear, devuelve el objeto para transformar futuros valores\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  QuantileTransformer de sklearn\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing. QuantileTransformer()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)   \n",
    "\n",
    "    def Transf_OneHot_binario(self, nombre_columna):\n",
    "        \"\"\"Crea objeto para transformacion  OneHot cuando la varible es binaria, devuelve\n",
    "        el transformador ya entrenado\"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_OneHot(self, nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  OneHot encoder de sklearn, devuelve el transformador entrenado \"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_woe(self,nombre_columna,nombre_target):\n",
    "        \"\"\"crea el objeto para Transformar los valores de la columna indicada  \n",
    "        usando woe de OptimalBinning,\n",
    "        Distinge de variables numericas y categoricas\n",
    "        Devuelve el objeto transoformador\"\"\"\n",
    "        try:\n",
    "            data_type = self.df[nombre_columna].dtypes\n",
    "            target = self.df[nombre_target]\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            if (data_type == 'object' or data_type.name == 'category'):  \n",
    "                optb = OptimalBinning(name = nombre_columna,dtype ='categorical',solver='mip')\n",
    "                optb.fit(x,target)\n",
    "            elif np.issubdtype(data_type, np.number):\n",
    "                optb = OptimalBinning(name = nombre_columna,dtype = 'numerical',solver='cp')\n",
    "                optb.fit(x,target)\n",
    "            else:\n",
    "                print(\"La variable se de convertir a tipo numerica o categorica\")\n",
    "                optb = None\n",
    "                \n",
    "            return optb\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)        \n",
    "\n",
    "    def calc_matriz_correlacion(self, columnas=None,filas=None):\n",
    "        \"\"\"metodo para la creacion de la matriz de correlacion, se pude ajustar el numero de filas y las columnas\n",
    "        a calcular para la correlacion\"\"\"\n",
    "        try:\n",
    "            if self.matriz_correlacion is None or self.matriz_correlacion.empty:\n",
    "                if columnas:\n",
    "                    # Se confirma que las columnas existan y sean numericas\n",
    "                    columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "                else:\n",
    "                    columnas = self.varibles_numericas()\n",
    "                if filas:\n",
    "                    data_set = self.df[columnas].sample(filas)\n",
    "                    data_set.reset_index(inplace=True,drop=True)\n",
    "                else:\n",
    "                    data_set = self.df[columnas]\n",
    "                self.matriz_correlacion = data_set.corr(method='pearson', numeric_only=True)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)    \n",
    "\n",
    "    def clasificacion_correlacion(self, target_name ,nivel_correlacion_target = 0.3, nivel_correlacion_inter_variables = 0.3):\n",
    "        \"\"\"Metodo para clasificar las variables en funcion del nivel de correlacion \n",
    "        1. Entre las variables y el objetivo (target_name), escogiendo las que tengan una \n",
    "        Moderada correlación |corr| > 0.3 pero se puede ajustar si se desea.\n",
    "        2. Entre las mismas variables permitiendo hasta una Moderada correlación |corr| < 0.3  entre ellas\n",
    "        con el objetivo de reducir la multicolinalidad\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Calculo de la correlacion para todas las variables\n",
    "            self.calc_matriz_correlacion()\n",
    "            matriz = self.matriz_correlacion\n",
    "\n",
    "            #Verificacion de la correlacion entre todas las variables y el target, ordenadas de mayor a menor\n",
    "            signals = []\n",
    "            for i in range(len(matriz)):\n",
    "                for j in range(i+1, len(matriz)):\n",
    "                    signal_1 = matriz.columns[i]\n",
    "                    signal_2 = matriz.columns[j]\n",
    "                    correlation = abs(matriz.iloc[i, j])\n",
    "                    signals.append([signal_1, signal_2, correlation])\n",
    "            df_correlacion = pd.DataFrame(signals, columns=['sig_1', 'sig_2', 'nivel_correlacion'])\n",
    "            df_correlacion_mod = df_correlacion[df_correlacion['sig_2'] == target_name].sort_values(by='nivel_correlacion', ascending=False)\n",
    "            variables_x = list(df_correlacion_mod [df_correlacion_mod['nivel_correlacion']>nivel_correlacion_target]['sig_1'])\n",
    "\n",
    "            # Verificacion de la correlacion entre variables \n",
    "            for i, variable1 in enumerate(variables_x):\n",
    "                for variable2 in variables_x[i+1:]:\n",
    "                    mask = (df_correlacion['sig_1'] == variable1) & (df_correlacion['sig_2'] == variable2)\n",
    "                    val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    if not val_corr:\n",
    "                        mask = (df_correlacion['sig_1'] == variable2) & (df_correlacion['sig_2'] == variable1)\n",
    "                        val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    val_corr = val_corr[0] if val_corr else 0\n",
    "                    if val_corr > nivel_correlacion_inter_variables:\n",
    "                        if variable2 in variables_x:\n",
    "                            variables_x.remove(variable2)\n",
    "            return variables_x\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    \n",
    "    def crea_EDAD_NORM(self,nombre_columna = 'EDAD'):\n",
    "        \"\"\"Para crear una variable EDAD normalizada se usa como referencia la fecha actual,\n",
    "        independiente del momento en que se corra el programa, se calcula la diferencia entre\n",
    "        la fecha de nacimiento provista y la fecha actual, se aplica una eliminacion de outliers\n",
    "        y una transformacion estandar normalizando entre el valor minimo y maximo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fecha_actual = datetime.datetime.now()\n",
    "            self.df[nombre_columna] = (fecha_actual - self.df['FECHANACIMIENTO']).dt.days // 365\n",
    "            self.eliminacion_outlier(nombre_columna)\n",
    "            escalador = self.Transf_MinMaxScaler(nombre_columna) \n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1)) \n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            self.df.drop(columns=['ANIO', 'MES','FECHANACIMIENTO'], inplace=True)\n",
    "        \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando el Transf_Quantile, \n",
    "        alamcena el nombre de la columna modificada y el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_Quantile(nombre_columna)\n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot_binario(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando OneHot_binario, \n",
    "        almacena el nombre de la columna modificada y \n",
    "        el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot_binario(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            self.df[nombre_columna] = arreglo.astype(int)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "    \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot(self,nombre_columna):\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            columnas_codificadas = escalador.get_feature_names_out([nombre_columna])\n",
    "            df_codificado = pd.DataFrame(arreglo, columns=columnas_codificadas)\n",
    "\n",
    "            #Se introducen en df los valores codificados\n",
    "            pos = self.df.columns.get_loc(nombre_columna)\n",
    "            for col in df_codificado.columns:\n",
    "                self.df.insert(pos, col, df_codificado[col])\n",
    "                pos += 1\n",
    "            self.df.drop(columns=[nombre_columna], inplace=True)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "        \n",
    "    def mod_woe(self,nombre_columna,nombre_target,metrica = \"woe\"):\n",
    "        \"\"\"Metodo para transformar los elementos de una columna \n",
    "        usando woe de OptimalBinning, se pueden utilizar otras metricas \n",
    "        como: \"event_rate\", \"woe\", \"indices\" and \"bins\" .\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_woe(nombre_columna,nombre_target)\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            self.df[nombre_columna] = escalador.transform(x, metric=metrica)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "\n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)  \n",
    "preproceso = preprocesamiento(revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b12e092-bee8-434e-bf03-bb13ec354712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_KNeighborsClassifier__n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.111830</td>\n",
       "      <td>0.103701</td>\n",
       "      <td>0.036024</td>\n",
       "      <td>1.414505e-03</td>\n",
       "      <td>6</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 6}</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.776824</td>\n",
       "      <td>0.845494</td>\n",
       "      <td>0.814277</td>\n",
       "      <td>0.028379</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.115473</td>\n",
       "      <td>0.121704</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>3.302009e-03</td>\n",
       "      <td>7</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 7}</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.871245</td>\n",
       "      <td>0.831450</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.173507</td>\n",
       "      <td>0.125247</td>\n",
       "      <td>0.037025</td>\n",
       "      <td>1.633264e-03</td>\n",
       "      <td>8</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 8}</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.798283</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.818593</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.109832</td>\n",
       "      <td>0.109358</td>\n",
       "      <td>0.035357</td>\n",
       "      <td>1.885987e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 9}</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.798283</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.828577</td>\n",
       "      <td>0.026418</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.091061</td>\n",
       "      <td>0.098866</td>\n",
       "      <td>0.034023</td>\n",
       "      <td>4.052337e-07</td>\n",
       "      <td>10</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 10}</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.824291</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.091106</td>\n",
       "      <td>0.102526</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>4.718200e-04</td>\n",
       "      <td>11</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 11}</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.862661</td>\n",
       "      <td>0.831438</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.101401</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>4.712018e-04</td>\n",
       "      <td>12</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 12}</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.858369</td>\n",
       "      <td>0.830007</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.093063</td>\n",
       "      <td>0.098161</td>\n",
       "      <td>0.037024</td>\n",
       "      <td>3.560934e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 13}</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.802575</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.827146</td>\n",
       "      <td>0.021092</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.091395</td>\n",
       "      <td>0.099469</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>4.716513e-04</td>\n",
       "      <td>14</td>\n",
       "      <td>{'KNeighborsClassifier__n_neighbors': 14}</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.806867</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.828577</td>\n",
       "      <td>0.019459</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.111830      0.103701         0.036024    1.414505e-03   \n",
       "1       1.115473      0.121704         0.036357    3.302009e-03   \n",
       "2       1.173507      0.125247         0.037025    1.633264e-03   \n",
       "3       1.109832      0.109358         0.035357    1.885987e-03   \n",
       "4       1.091061      0.098866         0.034023    4.052337e-07   \n",
       "5       1.091106      0.102526         0.034356    4.718200e-04   \n",
       "6       1.101401      0.105851         0.034356    4.712018e-04   \n",
       "7       1.093063      0.098161         0.037024    3.560934e-03   \n",
       "8       1.091395      0.099469         0.034690    4.716513e-04   \n",
       "\n",
       "  param_KNeighborsClassifier__n_neighbors  \\\n",
       "0                                       6   \n",
       "1                                       7   \n",
       "2                                       8   \n",
       "3                                       9   \n",
       "4                                      10   \n",
       "5                                      11   \n",
       "6                                      12   \n",
       "7                                      13   \n",
       "8                                      14   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "0   {'KNeighborsClassifier__n_neighbors': 6}           0.820513   \n",
       "1   {'KNeighborsClassifier__n_neighbors': 7}           0.816239   \n",
       "2   {'KNeighborsClassifier__n_neighbors': 8}           0.803419   \n",
       "3   {'KNeighborsClassifier__n_neighbors': 9}           0.824786   \n",
       "4  {'KNeighborsClassifier__n_neighbors': 10}           0.820513   \n",
       "5  {'KNeighborsClassifier__n_neighbors': 11}           0.824786   \n",
       "6  {'KNeighborsClassifier__n_neighbors': 12}           0.824786   \n",
       "7  {'KNeighborsClassifier__n_neighbors': 13}           0.824786   \n",
       "8  {'KNeighborsClassifier__n_neighbors': 14}           0.824786   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.776824           0.845494         0.814277        0.028379   \n",
       "1           0.806867           0.871245         0.831450        0.028398   \n",
       "2           0.798283           0.854077         0.818593        0.025179   \n",
       "3           0.798283           0.862661         0.828577        0.026418   \n",
       "4           0.789700           0.862661         0.824291        0.029906   \n",
       "5           0.806867           0.862661         0.831438        0.023258   \n",
       "6           0.806867           0.858369         0.830007        0.021347   \n",
       "7           0.802575           0.854077         0.827146        0.021092   \n",
       "8           0.806867           0.854077         0.828577        0.019459   \n",
       "\n",
       "   rank_test_score  \n",
       "0                9  \n",
       "1                1  \n",
       "2                8  \n",
       "3                5  \n",
       "4                7  \n",
       "5                2  \n",
       "6                3  \n",
       "7                6  \n",
       "8                4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproceso = preprocesamiento(revision)\n",
    "preproceso.df = preproceso.df.sample(1000)\n",
    "preproceso.crea_EDAD_NORM('EDAD')\n",
    "\n",
    "X = preproceso.df.drop(columns=['TARGET'])\n",
    "y = preproceso.df['TARGET'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)\n",
    "\n",
    "\n",
    "pipes_bining = []\n",
    "for columna in X_train.columns:\n",
    "    pipes_bining.append((columna + '_Tr', Pipeline([('columna', BinningProcess(variable_names=[columna]))]), [columna]))\n",
    "preprocessor = ColumnTransformer(pipes_bining)\n",
    "#preprocessor.fit(X_train, y_train)\n",
    "# X_train_transf = preprocessor.transform(X_train)\n",
    "\n",
    "pipe = Pipeline((\n",
    "    (\"BinningProcess\", preprocessor),\n",
    "    (\"Principal_component_analysis\", PCA(n_components=5)),\n",
    "    (\"KNeighborsClassifier\", KNeighborsClassifier(n_neighbors=5))))\n",
    "# pipe.fit(X_train, y_train)\n",
    "# prediccion= pipe.predict(X_test)\n",
    "\n",
    "\n",
    "# para revisar los parametros que se pueden utilizar en Grid\n",
    "# pipe.get_params()\n",
    "modelo_final = GridSearchCV(estimator = pipe,\n",
    "                     param_grid = {'KNeighborsClassifier__n_neighbors': list(range(6,15))},\n",
    "                     cv = 3)\n",
    "modelo_final.fit(X_train, y_train)           \n",
    "resultados = pd.DataFrame(modelo_final.cv_results_)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46216ac-c7fb-408e-bbb3-f28044cfb7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
