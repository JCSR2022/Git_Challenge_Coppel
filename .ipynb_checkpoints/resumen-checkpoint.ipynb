{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3aed19-9356-486b-bb31-5ed913b9b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Codigo hasta la fecha\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasgui import show\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "import statsmodels.api as sm\n",
    "from optbinning import OptimalBinning\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "print(\"librerias cargadas correctamente\") \n",
    "\n",
    "\n",
    "class revisor_data_csv:\n",
    "    \"\"\"Clase para carga y revisión de archivos con formato .csv\n",
    "    Con esto podemos visualizar y manipular la data de forma controlada minimizando errores y\n",
    "    pudiendo posteriormente crar un pipeline del proceso ETL\"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Constructor de la clase que recibe la ruta del archivo y carga el archivo\n",
    "        Se asume archivos .csv estándar, sep = \",\"\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.mensaje(\"cargando\", self.file_path)\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            self.mensaje(\"cargado\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def mensaje(self, tipo, arg=None):\n",
    "        tipos_mensajes = {\"cargando\": f\"Cargando archivo {arg}\",\n",
    "                          \"cargado\": \"Carga completa\"}\n",
    "        if tipo in tipos_mensajes:\n",
    "            clear_output(wait=True)\n",
    "            print(tipos_mensajes[tipo])\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Tipo de mensaje no válido\")\n",
    "            print(f\"{list(tipos_mensajes.keys())}\")\n",
    "    \n",
    "    def analisis_nulos(self):\n",
    "        cuenta_nulos = self.df.isnull().sum()\n",
    "        cuenta_nulos = cuenta_nulos[cuenta_nulos != 0]\n",
    "        porcentaje_nulos = round((cuenta_nulos / self.df.shape[0]) * 100, 2)\n",
    "        return pd.DataFrame({\"cant_nulos\": cuenta_nulos, \"porcentaje_nulos\": porcentaje_nulos})\n",
    "\n",
    "    def nulos_EliminacionSimple(self, lista):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores nulos en las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.dropna(subset=lista)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def nulos_MediaCondicional(self, variable, lista):\n",
    "        \"\"\"Se asignará a los valores nulos de la columna variable,\n",
    "        el valor promedio agrupando con las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df.loc[self.df[variable].isnull(), variable] = self.df.groupby(lista, as_index=False)[[variable]].transform('mean')\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def eliminar_por_condicion(self,columna,condicion):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores 'condicion' en la columna indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.drop(self.df[self.df[columna] == condicion].index)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "\n",
    "#revision = revisor_data_csv(\"..\\\\datos_internos/20210513_Challenge_AA_MANANA.csv\")\n",
    "revision = revisor_data_csv(\"df_prueba_MANANA.csv\")\n",
    "\n",
    "            \n",
    "# Eliminacion de valores nulos\n",
    "revision.nulos_EliminacionSimple(['MORAS', 'SEXO', 'ESTADOCIVIL', 'FECHANACIMIENTO', 'ANTIGUEDAD', 'EDAD'])\n",
    "revision.nulos_MediaCondicional('INGRESO',['ESTADO','EDAD','ESTADOCIVIL','SEXO'])\n",
    "revision.nulos_EliminacionSimple(['INGRESO'])\n",
    "\n",
    "\n",
    "# correciones de tipo en datos:\n",
    "revision.df['MORAS'] = revision.df['MORAS'].astype(int)\n",
    "revision.df['ANTIGUEDAD'] = revision.df['ANTIGUEDAD'].astype(int)\n",
    "revision.df['EDAD'] = revision.df['EDAD'].astype(int)\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO']).dt.date\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO'])\n",
    "revision.eliminar_por_condicion('ESTADOCIVIL',' ')\n",
    "\n",
    "\n",
    "class revisor_grafico:\n",
    "    \"\"\"Clase para contener todos las formas de revisión grafica de un dataframe que se desee\n",
    "    Con esto podemos crear y agregar diferentes métodos en función de lo que queramos visualizar\"\"\"\n",
    "    def __init__(self,df):\n",
    "        \"\"\"\n",
    "        Constructor de la clase que recibe el df\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se carga el df \n",
    "               self.df = df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def histograma_general(self,columnas=None,filas=None):\n",
    "        \"\"\"Grafica los histogramas de las variables en el df:\n",
    "            -si no se indica ningun parametro grafica todas las columnas y filas posibles\n",
    "            - Si se introduce una lista con las columnas a graficar se grafican solo las indicadas\n",
    "            - Si se introduce un integer con la cantidad de filas a utlizar se hace un smpling con n filas\"\"\"\n",
    "        try:\n",
    "            if columnas:\n",
    "                # Se confirma que las columnas existan, se obvian columnas que no existan en df\n",
    "                columnas = [ col for col in columnas if col in self.df.columns]\n",
    "            else:\n",
    "                columnas = self.df.columns\n",
    "            if filas:\n",
    "                self.df[columnas].sample(filas).hist(figsize=(10, 10))\n",
    "            else:\n",
    "                self.df[columnas].hist(figsize=(10, 10))\n",
    "            plt.show() \n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def boxplot_general(self,columnas=None,filas=None):\n",
    "        \"\"\"Grafica Box plots de las variables en el df:\n",
    "            -si no se indica ningun parametro grafica todas las columnas y filas posibles\n",
    "            - Si se introduce una lista con las columnas a graficar se grafican solo las indicadas\n",
    "            - Si se introduce un integer con la cantidad de filas a utlizar se hace un smpling con n filas\"\"\"\n",
    "        try:\n",
    "            if columnas:\n",
    "                # Se confirma que las columnas existan, se obvian columnas que no existan en df\n",
    "                columnas = [ col for col in columnas if col in self.df.columns]\n",
    "            else:\n",
    "                columnas = self.df.columns\n",
    "            if filas:\n",
    "                self.df[columnas].sample(filas).plot(kind='box', subplots=True, layout=(3, 4), sharex=False, sharey=False, figsize=(10, 10))\n",
    "            else:\n",
    "                self.df[columnas].plot(kind='box', subplots=True, layout=(3, 4), sharex=False, sharey=False, figsize=(10, 10))\n",
    "            plt.show() \n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "\n",
    "    def histograma_frecuencia(self, columna, n):\n",
    "        \"\"\"Crea un histograma de frecuencia para una columna del dataframe utilizando una muestra de n registros\"\"\"\n",
    "        try:\n",
    "            dataset = self.df.sample(n)\n",
    "            if dataset[columna].dtype == 'object':\n",
    "                conteo = dataset[columna].value_counts().reset_index()\n",
    "                conteo.columns = [columna, 'conteo']\n",
    "                conteo['conteo_%'] = round(conteo['conteo'] / sum(conteo['conteo']) * 100, 1)\n",
    "                plt.bar(conteo[columna], conteo['conteo_%'])\n",
    "                plt.xlabel(columna)\n",
    "                plt.ylabel('Frecuencia_%')\n",
    "                plt.title(f'Frecuencia de variable {columna} para {n} registros')\n",
    "            else:\n",
    "                data_set = dataset[columna]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.histplot(data=data_set, kde=True, bins=20, alpha=0.7)\n",
    "                media = data_set.mean()\n",
    "                moda = data_set.mode().values[0]\n",
    "                mediana = data_set.median()\n",
    "                plt.axvline(media, color='red', linestyle='--', label=f'Media: {media:.2f}')\n",
    "                plt.axvline(moda, color='green', linestyle='--', label=f'Moda: {moda:.2f}')\n",
    "                plt.axvline(mediana, color='blue', linestyle='--', label=f'Mediana: {mediana:.2f}')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.title(f'Histograma de {columna} para {n} registros')\n",
    "                plt.xlabel(f'Valores para {columna}')\n",
    "                plt.ylabel(f'Frecuencias')\n",
    "            plt.show()\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def histograma_dinamico(self, columna, max_registros):\n",
    "        \"\"\"Crea histogramas de frecuencia dinámicos para una columna del dataframe utilizando diferentes muestras de registros\"\"\"\n",
    "        try:\n",
    "            cantidades = [int(np.exp(i)) for i in np.linspace(np.log(100), np.log(max_registros), 10)]\n",
    "            for cant in cantidades:\n",
    "                clear_output(wait=True)\n",
    "                self.histograma_frecuencia(columna, cant)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def varibles_numericas(self):\n",
    "        \"\"\"Devuelve una lista con las variables numericas de un df\"\"\"\n",
    "        return [column_name for column_name, data_type in zip(self.df.columns, self.df.dtypes) if ((data_type != 'category') and  \n",
    "                                                                                                   np.issubdtype(data_type, np.number))]\n",
    "\n",
    "    def pairplot(self,columnas=None,filas=None):\n",
    "        \"\"\"Generará una matriz de gráficos de dispersión, donde cada gráfico mostrará la relación entre dos columnas del DataFrame. \n",
    "        La diagonal principal de la matriz mostrará un histograma de cada columna, solo funcionará con columnas numéricas.\n",
    "            -si no se indica ningun parametro grafica todas las columnas y filas posibles\n",
    "            - Si se introduce una lista con las columnas a graficar se grafican solo las indicadas\n",
    "            - Si se introduce un integer con la cantidad de filas a utlizar se hace un smpling con n filas\"\"\"\n",
    "        try:\n",
    "            if columnas:\n",
    "                # Se confirma que las columnas existan y sean numericas\n",
    "                columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "            else:\n",
    "                columnas = self.varibles_numericas()\n",
    "            if filas:\n",
    "                data_set = self.df[columnas].sample(filas)\n",
    "                data_set.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                data_set = self.df[columnas]\n",
    "            sns.pairplot(data_set)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def detalle_scatterplot(self, x, y,filas=None,hue=None, style=None,size=None):\n",
    "        try:\n",
    "            if filas:\n",
    "                data_set = self.df.sample(filas)\n",
    "                data_set.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                data_set = self.df\n",
    "            sns.scatterplot(data=data_set, x=x, y=y, hue=hue,style=style,size=size)\n",
    "            plt.show()\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def detalle_boxplot(self, columna,filas=None, categoria1=None, categoria2=None):\n",
    "        if filas:\n",
    "            data_set = self.df.sample(filas)\n",
    "            data_set.reset_index(inplace=True,drop=True)\n",
    "        else:\n",
    "            data_set = self.df\n",
    "\n",
    "        \n",
    "        if categoria1:\n",
    "            if categoria2:\n",
    "                sns.boxplot(x=categoria1, y=columna, hue=categoria2, data=data_set, palette='CMRmap')\n",
    "            else:\n",
    "                sns.boxplot(x=categoria1, y=columna, data=data_set)\n",
    "        else:\n",
    "            sns.boxplot(y=columna, data=data_set, width=0.2, showmeans=True, linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "    def correlacion(self, columnas=None,filas=None):\n",
    "        try:\n",
    "            if columnas:\n",
    "                # Se confirma que las columnas existan y sean numericas\n",
    "                columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "            else:\n",
    "                columnas = self.varibles_numericas()\n",
    "            if filas:\n",
    "                data_set = self.df[columnas].sample(filas)\n",
    "                data_set.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                data_set = self.df[columnas]\n",
    "\n",
    "            correlacion = data_set.corr(method='pearson', numeric_only=True)\n",
    "            plt.figure(figsize=(9,9))\n",
    "            sns.heatmap(correlacion,cmap = 'RdBu',vmin=-1, vmax=1, square=True,  annot = True,fmt=\".1f\",annot_kws = {'fontsize':11, 'fontweight':'bold'})\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "\n",
    "revision.df = revision.df.drop(['CLIENTE'], axis=1)\n",
    "graficas = revisor_grafico(revision.df)\n",
    "\n",
    "\n",
    "class preprocesamiento:\n",
    "    def __init__(self,revisor_data):\n",
    "        \"\"\"\n",
    "        Constructor de la clase recibe el objeto instanciado de la clase revisor_data_csv, \n",
    "        seria mejor usar un patron de diseño DataFrameSingleton pero esta division solo es \n",
    "        para fines demostrativos,en codigo final todos los metodos deben \n",
    "        pertenecer a una sola clase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.revisor_data = revisor_data\n",
    "            self.df = self.revisor_data.df\n",
    "            self.matriz_correlacion = None\n",
    "            self.transformaciones = {}\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    #Este metodo esta repetido en la clase graficos, al final se debe mejorar el codigo ya sea uniendo todas las clases o usando herencias.\n",
    "    def varibles_numericas(self):\n",
    "        \"\"\"Devuelve una lista con las variables numericas de un df\"\"\"\n",
    "        return [column_name for column_name, data_type in zip(self.df.columns, self.df.dtypes) if ((data_type != 'category') and  np.issubdtype(data_type, np.number))]\n",
    "    \n",
    "    def detecion_outlier(self,nombre_columna,q=.1):\n",
    "        \"\"\" \n",
    "        Funcion para detectar valores atípicos (utiliers) de una columna especifica usando cuantiles.\n",
    "        Por defecto se usa Deciles / dividiendo la distribucion en 10 partes, pero ajsutando el valor de\n",
    "          q = 0.25 se trabajaria con cuartiles.\n",
    "        La funcion devuelve una \"lista/pandas.core.indexes.numeric.Int64Index\"  con los indices de los \n",
    "        valores atípicos del df\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #calculo de cuantiles\n",
    "        Q1 = self.df[nombre_columna].quantile(q)\n",
    "        Q3 = self.df[nombre_columna].quantile(1-q)\n",
    "        IQR = Q3-Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        indice_filas_eliminar = self.df.index[(self.df[nombre_columna] < limite_inferior) | (self.df[nombre_columna] > limite_superior) ]\n",
    "        return indice_filas_eliminar   \n",
    "\n",
    "    def eliminacion_outlier(self,nombre_columna,q=0.1):\n",
    "        \"\"\" \n",
    "        Funcion para eliminar valores atípicos (outliers) de una columna especifica usando cuantiles.\n",
    "        La funcion no devuelve nada porque los cambios se hacen en el df que se pasa por referencia\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                indices = self.detecion_outlier(nombre_columna,q)\n",
    "                self.df = self.df.drop(indices)\n",
    "                self.df.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def guardar_transformador(self,nombre_columna,transformador):\n",
    "        \"\"\"Metodo para guardar transformador aplicado, se almacenan en lista, si se\n",
    "        aplican dos o mas se almacenaran en la secuencia aplicada\"\"\"\n",
    "        if nombre_columna in list(self.transformaciones.keys()):\n",
    "            self.transformaciones[nombre_columna] = self.transformaciones[nombre_columna]+[transformador]\n",
    "        else:\n",
    "            self.transformaciones[nombre_columna] = [transformador]\n",
    "            \n",
    "    def Transf_MinMaxScaler(self,nombre_columna):\n",
    "        \"\"\"crea objeto que Transforma los valores de la columna indicada \n",
    "        usando MinMaxScaler de sklear, devuelve el objeto para transformar futuros valores\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  QuantileTransformer de sklearn\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing. QuantileTransformer()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)   \n",
    "\n",
    "    def Transf_OneHot_binario(self, nombre_columna):\n",
    "        \"\"\"Crea objeto para transformacion  OneHot cuando la varible es binaria, devuelve\n",
    "        el transformador ya entrenado\"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_OneHot(self, nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  OneHot encoder de sklearn, devuelve el transformador entrenado \"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_woe(self,nombre_columna,nombre_target):\n",
    "        \"\"\"crea el objeto para Transformar los valores de la columna indicada  \n",
    "        usando woe de OptimalBinning,\n",
    "        Distinge de variables numericas y categoricas\n",
    "        Devuelve el objeto transoformador\"\"\"\n",
    "        try:\n",
    "            data_type = self.df[nombre_columna].dtypes\n",
    "            target = self.df[nombre_target]\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            if (data_type == 'object' or data_type.name == 'category'):  \n",
    "                optb = OptimalBinning(name = nombre_columna,dtype ='categorical',solver='mip')\n",
    "                optb.fit(x,target)\n",
    "            elif np.issubdtype(data_type, np.number):\n",
    "                optb = OptimalBinning(name = nombre_columna,dtype = 'numerical',solver='cp')\n",
    "                optb.fit(x,target)\n",
    "            else:\n",
    "                print(\"La variable se de convertir a tipo numerica o categorica\")\n",
    "                optb = None\n",
    "                \n",
    "            return optb\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)        \n",
    "\n",
    "    def calc_matriz_correlacion(self, columnas=None,filas=None):\n",
    "        \"\"\"metodo para la creacion de la matriz de correlacion, se pude ajustar el numero de filas y las columnas\n",
    "        a calcular para la correlacion\"\"\"\n",
    "        try:\n",
    "            if self.matriz_correlacion is None or self.matriz_correlacion.empty:\n",
    "                if columnas:\n",
    "                    # Se confirma que las columnas existan y sean numericas\n",
    "                    columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "                else:\n",
    "                    columnas = self.varibles_numericas()\n",
    "                if filas:\n",
    "                    data_set = self.df[columnas].sample(filas)\n",
    "                    data_set.reset_index(inplace=True,drop=True)\n",
    "                else:\n",
    "                    data_set = self.df[columnas]\n",
    "                self.matriz_correlacion = data_set.corr(method='pearson', numeric_only=True)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)    \n",
    "\n",
    "    def clasificacion_correlacion(self, target_name ,nivel_correlacion_target = 0.3, nivel_correlacion_inter_variables = 0.3):\n",
    "        \"\"\"Metodo para clasificar las variables en funcion del nivel de correlacion \n",
    "        1. Entre las variables y el objetivo (target_name), escogiendo las que tengan una \n",
    "        Moderada correlación |corr| > 0.3 pero se puede ajustar si se desea.\n",
    "        2. Entre las mismas variables permitiendo hasta una Moderada correlación |corr| < 0.3  entre ellas\n",
    "        con el objetivo de reducir la multicolinalidad\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Calculo de la correlacion para todas las variables\n",
    "            self.calc_matriz_correlacion()\n",
    "            matriz = self.matriz_correlacion\n",
    "\n",
    "            #Verificacion de la correlacion entre todas las variables y el target, ordenadas de mayor a menor\n",
    "            signals = []\n",
    "            for i in range(len(matriz)):\n",
    "                for j in range(i+1, len(matriz)):\n",
    "                    signal_1 = matriz.columns[i]\n",
    "                    signal_2 = matriz.columns[j]\n",
    "                    correlation = abs(matriz.iloc[i, j])\n",
    "                    signals.append([signal_1, signal_2, correlation])\n",
    "            df_correlacion = pd.DataFrame(signals, columns=['sig_1', 'sig_2', 'nivel_correlacion'])\n",
    "            df_correlacion_mod = df_correlacion[df_correlacion['sig_2'] == target_name].sort_values(by='nivel_correlacion', ascending=False)\n",
    "            variables_x = list(df_correlacion_mod [df_correlacion_mod['nivel_correlacion']>nivel_correlacion_target]['sig_1'])\n",
    "\n",
    "            # Verificacion de la correlacion entre variables \n",
    "            for i, variable1 in enumerate(variables_x):\n",
    "                for variable2 in variables_x[i+1:]:\n",
    "                    mask = (df_correlacion['sig_1'] == variable1) & (df_correlacion['sig_2'] == variable2)\n",
    "                    val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    if not val_corr:\n",
    "                        mask = (df_correlacion['sig_1'] == variable2) & (df_correlacion['sig_2'] == variable1)\n",
    "                        val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    val_corr = val_corr[0] if val_corr else 0\n",
    "                    if val_corr > nivel_correlacion_inter_variables:\n",
    "                        if variable2 in variables_x:\n",
    "                            variables_x.remove(variable2)\n",
    "            return variables_x\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    \n",
    "    def crea_EDAD_NORM(self,nombre_columna = 'EDAD'):\n",
    "        \"\"\"Para crear una variable EDAD normalizada se usa como referencia la fecha actual,\n",
    "        independiente del momento en que se corra el programa, se calcula la diferencia entre\n",
    "        la fecha de nacimiento provista y la fecha actual, se aplica una eliminacion de outliers\n",
    "        y una transformacion estandar normalizando entre el valor minimo y maximo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fecha_actual = datetime.datetime.now()\n",
    "            self.df[nombre_columna] = (fecha_actual - self.df['FECHANACIMIENTO']).dt.days // 365\n",
    "            self.eliminacion_outlier(nombre_columna)\n",
    "            escalador = self.Transf_MinMaxScaler(nombre_columna) \n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1)) \n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            self.df.drop(columns=['ANIO', 'MES','FECHANACIMIENTO'], inplace=True)\n",
    "        \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando el Transf_Quantile, \n",
    "        alamcena el nombre de la columna modificada y el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_Quantile(nombre_columna)\n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot_binario(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando OneHot_binario, \n",
    "        almacena el nombre de la columna modificada y \n",
    "        el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot_binario(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            self.df[nombre_columna] = arreglo.astype(int)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "    \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot(self,nombre_columna):\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            columnas_codificadas = escalador.get_feature_names_out([nombre_columna])\n",
    "            df_codificado = pd.DataFrame(arreglo, columns=columnas_codificadas)\n",
    "\n",
    "            #Se introducen en df los valores codificados\n",
    "            pos = self.df.columns.get_loc(nombre_columna)\n",
    "            for col in df_codificado.columns:\n",
    "                self.df.insert(pos, col, df_codificado[col])\n",
    "                pos += 1\n",
    "            self.df.drop(columns=[nombre_columna], inplace=True)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "        \n",
    "    def mod_woe(self,nombre_columna,nombre_target,metrica = \"woe\"):\n",
    "        \"\"\"Metodo para transformar los elementos de una columna \n",
    "        usando woe de OptimalBinning, se pueden utilizar otras metricas \n",
    "        como: \"event_rate\", \"woe\", \"indices\" and \"bins\" .\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_woe(nombre_columna,nombre_target)\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            self.df[nombre_columna] = escalador.transform(x, metric=metrica)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "\n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)  \n",
    "\n",
    "preproceso = preprocesamiento(revision)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874b2f94-b9cc-4381-b251-69abbf74ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocesamiento:\n",
    "    def __init__(self,revisor_data):\n",
    "        \"\"\"\n",
    "        Constructor de la clase recibe el objeto instanciado de la clase revisor_data_csv, \n",
    "        seria mejor usar un patron de diseño DataFrameSingleton pero esta division solo es \n",
    "        para fines demostrativos,en codigo final todos los metodos deben \n",
    "        pertenecer a una sola clase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.revisor_data = revisor_data\n",
    "            self.df = self.revisor_data.df\n",
    "            self.matriz_correlacion = None\n",
    "            self.transformaciones = {}\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    #Este metodo esta repetido en la clase graficos, al final se debe mejorar el codigo ya sea uniendo todas las clases o usando herencias.\n",
    "    def varibles_numericas(self):\n",
    "        \"\"\"Devuelve una lista con las variables numericas de un df\"\"\"\n",
    "        return [column_name for column_name, data_type in zip(self.df.columns, self.df.dtypes) if ((data_type != 'category') and  np.issubdtype(data_type, np.number))]\n",
    "    \n",
    "    def detecion_outlier(self,nombre_columna,q=.1):\n",
    "        \"\"\" \n",
    "        Funcion para detectar valores atípicos (utiliers) de una columna especifica usando cuantiles.\n",
    "        Por defecto se usa Deciles / dividiendo la distribucion en 10 partes, pero ajsutando el valor de\n",
    "          q = 0.25 se trabajaria con cuartiles.\n",
    "        La funcion devuelve una \"lista/pandas.core.indexes.numeric.Int64Index\"  con los indices de los \n",
    "        valores atípicos del df\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #calculo de cuantiles\n",
    "        Q1 = self.df[nombre_columna].quantile(q)\n",
    "        Q3 = self.df[nombre_columna].quantile(1-q)\n",
    "        IQR = Q3-Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        indice_filas_eliminar = self.df.index[(self.df[nombre_columna] < limite_inferior) | (self.df[nombre_columna] > limite_superior) ]\n",
    "        return indice_filas_eliminar   \n",
    "\n",
    "    def eliminacion_outlier(self,nombre_columna,q=0.1):\n",
    "        \"\"\" \n",
    "        Funcion para eliminar valores atípicos (outliers) de una columna especifica usando cuantiles.\n",
    "        La funcion no devuelve nada porque los cambios se hacen en el df que se pasa por referencia\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                indices = self.detecion_outlier(nombre_columna,q)\n",
    "                self.df = self.df.drop(indices)\n",
    "                self.df.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def guardar_transformador(self,nombre_columna,transformador):\n",
    "        \"\"\"Metodo para guardar transformador aplicado, se almacenan en lista, si se\n",
    "        aplican dos o mas se almacenaran en la secuencia aplicada\"\"\"\n",
    "        if nombre_columna in list(self.transformaciones.keys()):\n",
    "            self.transformaciones[nombre_columna] = self.transformaciones[nombre_columna]+[transformador]\n",
    "        else:\n",
    "            self.transformaciones[nombre_columna] = [transformador]\n",
    "            \n",
    "    def Transf_MinMaxScaler(self,nombre_columna):\n",
    "        \"\"\"crea objeto que Transforma los valores de la columna indicada \n",
    "        usando MinMaxScaler de sklear, devuelve el objeto para transformar futuros valores\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  QuantileTransformer de sklearn\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing. QuantileTransformer()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)   \n",
    "\n",
    "    def Transf_OneHot_binario(self, nombre_columna):\n",
    "        \"\"\"Crea objeto para transformacion  OneHot cuando la varible es binaria, devuelve\n",
    "        el transformador ya entrenado\"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_OneHot(self, nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  OneHot encoder de sklearn, devuelve el transformador entrenado \"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_woe(self,nombre_columna,nombre_target):\n",
    "        \"\"\"crea el objeto para Transformar los valores de la columna indicada  \n",
    "        usando woe de OptimalBinning,\n",
    "        Distinge de variables numericas y categoricas\n",
    "        Devuelve el objeto transoformador\"\"\"\n",
    "        try:\n",
    "            data_type = self.df[nombre_columna].dtypes\n",
    "            target = self.df[nombre_target]\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            if (data_type == 'object' or data_type.name == 'category'):  \n",
    "                optb = OptimalBinning(name = nombre_columna,dtype ='categorical',solver='mip')\n",
    "                optb.fit(x,target)\n",
    "            elif np.issubdtype(data_type, np.number):\n",
    "                optb = OptimalBinning(name = nombre_columna,dtype = 'numerical',solver='cp')\n",
    "                optb.fit(x,target)\n",
    "            else:\n",
    "                print(\"La variable se de convertir a tipo numerica o categorica\")\n",
    "                optb = None\n",
    "                \n",
    "            return optb\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)        \n",
    "\n",
    "    def calc_matriz_correlacion(self, columnas=None,filas=None):\n",
    "        \"\"\"metodo para la creacion de la matriz de correlacion, se pude ajustar el numero de filas y las columnas\n",
    "        a calcular para la correlacion\"\"\"\n",
    "        try:\n",
    "            if self.matriz_correlacion is None or self.matriz_correlacion.empty:\n",
    "                if columnas:\n",
    "                    # Se confirma que las columnas existan y sean numericas\n",
    "                    columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "                else:\n",
    "                    columnas = self.varibles_numericas()\n",
    "                if filas:\n",
    "                    data_set = self.df[columnas].sample(filas)\n",
    "                    data_set.reset_index(inplace=True,drop=True)\n",
    "                else:\n",
    "                    data_set = self.df[columnas]\n",
    "                self.matriz_correlacion = data_set.corr(method='pearson', numeric_only=True)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)    \n",
    "\n",
    "    def clasificacion_correlacion(self, target_name ,nivel_correlacion_target = 0.3, nivel_correlacion_inter_variables = 0.3):\n",
    "        \"\"\"Metodo para clasificar las variables en funcion del nivel de correlacion \n",
    "        1. Entre las variables y el objetivo (target_name), escogiendo las que tengan una \n",
    "        Moderada correlación |corr| > 0.3 pero se puede ajustar si se desea.\n",
    "        2. Entre las mismas variables permitiendo hasta una Moderada correlación |corr| < 0.3  entre ellas\n",
    "        con el objetivo de reducir la multicolinalidad\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Calculo de la correlacion para todas las variables\n",
    "            self.calc_matriz_correlacion()\n",
    "            matriz = self.matriz_correlacion\n",
    "\n",
    "            #Verificacion de la correlacion entre todas las variables y el target, ordenadas de mayor a menor\n",
    "            signals = []\n",
    "            for i in range(len(matriz)):\n",
    "                for j in range(i+1, len(matriz)):\n",
    "                    signal_1 = matriz.columns[i]\n",
    "                    signal_2 = matriz.columns[j]\n",
    "                    correlation = abs(matriz.iloc[i, j])\n",
    "                    signals.append([signal_1, signal_2, correlation])\n",
    "            df_correlacion = pd.DataFrame(signals, columns=['sig_1', 'sig_2', 'nivel_correlacion'])\n",
    "            df_correlacion_mod = df_correlacion[df_correlacion['sig_2'] == target_name].sort_values(by='nivel_correlacion', ascending=False)\n",
    "            variables_x = list(df_correlacion_mod [df_correlacion_mod['nivel_correlacion']>nivel_correlacion_target]['sig_1'])\n",
    "\n",
    "            # Verificacion de la correlacion entre variables \n",
    "            for i, variable1 in enumerate(variables_x):\n",
    "                for variable2 in variables_x[i+1:]:\n",
    "                    mask = (df_correlacion['sig_1'] == variable1) & (df_correlacion['sig_2'] == variable2)\n",
    "                    val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    if not val_corr:\n",
    "                        mask = (df_correlacion['sig_1'] == variable2) & (df_correlacion['sig_2'] == variable1)\n",
    "                        val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    val_corr = val_corr[0] if val_corr else 0\n",
    "                    if val_corr > nivel_correlacion_inter_variables:\n",
    "                        if variable2 in variables_x:\n",
    "                            variables_x.remove(variable2)\n",
    "            return variables_x\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    \n",
    "    def crea_EDAD_NORM(self,nombre_columna = 'EDAD'):\n",
    "        \"\"\"Para crear una variable EDAD normalizada se usa como referencia la fecha actual,\n",
    "        independiente del momento en que se corra el programa, se calcula la diferencia entre\n",
    "        la fecha de nacimiento provista y la fecha actual, se aplica una eliminacion de outliers\n",
    "        y una transformacion estandar normalizando entre el valor minimo y maximo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fecha_actual = datetime.datetime.now()\n",
    "            self.df[nombre_columna] = (fecha_actual - self.df['FECHANACIMIENTO']).dt.days // 365\n",
    "            self.eliminacion_outlier(nombre_columna)\n",
    "            escalador = self.Transf_MinMaxScaler(nombre_columna) \n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1)) \n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            self.df.drop(columns=['ANIO', 'MES','FECHANACIMIENTO'], inplace=True)\n",
    "        \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando el Transf_Quantile, \n",
    "        alamcena el nombre de la columna modificada y el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_Quantile(nombre_columna)\n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot_binario(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando OneHot_binario, \n",
    "        almacena el nombre de la columna modificada y \n",
    "        el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot_binario(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            self.df[nombre_columna] = arreglo.astype(int)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "    \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot(self,nombre_columna):\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            columnas_codificadas = escalador.get_feature_names_out([nombre_columna])\n",
    "            df_codificado = pd.DataFrame(arreglo, columns=columnas_codificadas)\n",
    "\n",
    "            #Se introducen en df los valores codificados\n",
    "            pos = self.df.columns.get_loc(nombre_columna)\n",
    "            for col in df_codificado.columns:\n",
    "                self.df.insert(pos, col, df_codificado[col])\n",
    "                pos += 1\n",
    "            self.df.drop(columns=[nombre_columna], inplace=True)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "        \n",
    "    def mod_woe(self,nombre_columna,nombre_target,metrica = \"woe\"):\n",
    "        \"\"\"Metodo para transformar los elementos de una columna \n",
    "        usando woe de OptimalBinning, se pueden utilizar otras metricas \n",
    "        como: \"event_rate\", \"woe\", \"indices\" and \"bins\" .\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_woe(nombre_columna,nombre_target)\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            self.df[nombre_columna] = escalador.transform(x, metric=metrica)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "\n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)  \n",
    "\n",
    "preproceso = preprocesamiento(revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed3c70-e942-4e24-bdaf-90788f049162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
