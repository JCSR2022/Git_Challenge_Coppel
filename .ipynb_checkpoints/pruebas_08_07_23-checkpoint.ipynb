{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4598f57-a65e-42d5-9fe4-18c5612ecdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa786476-b75c-49db-a6eb-c4db1a0cf5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga completa\n"
     ]
    }
   ],
   "source": [
    "#Resumen\n",
    "\n",
    "# Librerias a utilizar, por defecto ya vienen instaladas en el entorno \"pip install git jupyterlab numpy pandas plotly \"\n",
    "# para mejor visualizacion de los df y manejo de estadisticas:\n",
    "# ! pip install pandasgui\n",
    "# ! pip install pandas-profiling\n",
    "# ! pip install seaborn\n",
    "# ! pip install scipy\n",
    "#! pip install optbinning\n",
    "#! pip install ortools == 9.4.1874\n",
    "#! pip install -U scikit-learn\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasgui import show\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "import statsmodels.api as sm\n",
    "from optbinning import OptimalBinning\n",
    "from optbinning import BinningProcess\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score , recall_score, f1_score, make_scorer \n",
    "print(\"librerias cargadas correctamente\") \n",
    "\n",
    "class revisor_data_csv:\n",
    "    \"\"\"Clase para carga y revisión de archivos con formato .csv\n",
    "    Con esto podemos visualizar y manipular la data de forma controlada minimizando errores y\n",
    "    pudiendo posteriormente crar un pipeline del proceso ETL\"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Constructor de la clase que recibe la ruta del archivo y carga el archivo\n",
    "        Se asume archivos .csv estándar, sep = \",\"\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.mensaje(\"cargando\", self.file_path)\n",
    "            self.df = pd.read_csv(self.file_path)\n",
    "            self.mensaje(\"cargado\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def mensaje(self, tipo, arg=None):\n",
    "        tipos_mensajes = {\"cargando\": f\"Cargando archivo {arg}\",\n",
    "                          \"cargado\": \"Carga completa\"}\n",
    "        if tipo in tipos_mensajes:\n",
    "            clear_output(wait=True)\n",
    "            print(tipos_mensajes[tipo])\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Tipo de mensaje no válido\")\n",
    "            print(f\"{list(tipos_mensajes.keys())}\")\n",
    "    \n",
    "    def analisis_nulos(self):\n",
    "        cuenta_nulos = self.df.isnull().sum()\n",
    "        cuenta_nulos = cuenta_nulos[cuenta_nulos != 0]\n",
    "        porcentaje_nulos = round((cuenta_nulos / self.df.shape[0]) * 100, 2)\n",
    "        return pd.DataFrame({\"cant_nulos\": cuenta_nulos, \"porcentaje_nulos\": porcentaje_nulos})\n",
    "\n",
    "    def nulos_EliminacionSimple(self, lista):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores nulos en las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.dropna(subset=lista)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def nulos_MediaCondicional(self, variable, lista):\n",
    "        \"\"\"Se asignará a los valores nulos de la columna variable,\n",
    "        el valor promedio agrupando con las columnas indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df.loc[self.df[variable].isnull(), variable] = self.df.groupby(lista, as_index=False)[[variable]].transform('mean')\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def eliminar_por_condicion(self,columna,condicion):\n",
    "        \"\"\"Se eliminarán las filas que contengan valores 'condicion' en la columna indicadas por la lista\"\"\"\n",
    "        try:\n",
    "            self.df = self.df.drop(self.df[self.df[columna] == condicion].index)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "revision = revisor_data_csv(\"..\\\\datos_internos/20210513_Challenge_AA_MANANA.csv\")\n",
    "#revision = revisor_data_csv(\"df_prueba_MANANA.csv\")\n",
    "\n",
    "# Eliminacion de valores nulos\n",
    "revision.nulos_EliminacionSimple(['MORAS', 'SEXO', 'ESTADOCIVIL', 'FECHANACIMIENTO', 'ANTIGUEDAD', 'EDAD'])\n",
    "revision.nulos_MediaCondicional('INGRESO',['ESTADO','EDAD','ESTADOCIVIL','SEXO'])\n",
    "revision.nulos_EliminacionSimple(['INGRESO'])\n",
    "revision.df['MORAS'] = revision.df['MORAS'].astype(int)\n",
    "revision.df['ANTIGUEDAD'] = revision.df['ANTIGUEDAD'].astype(int)\n",
    "revision.df['EDAD'] = revision.df['EDAD'].astype(int)\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO']).dt.date\n",
    "revision.df['FECHANACIMIENTO'] = pd.to_datetime(revision.df['FECHANACIMIENTO'])\n",
    "revision.eliminar_por_condicion('ESTADOCIVIL',' ')\n",
    "\n",
    "revision.df = revision.df.drop(['CLIENTE'], axis=1)\n",
    "\n",
    "class preprocesamiento:\n",
    "    def __init__(self,revisor_data):\n",
    "        \"\"\"\n",
    "        Constructor de la clase recibe el objeto instanciado de la clase revisor_data_csv, \n",
    "        seria mejor usar un patron de diseño DataFrameSingleton pero esta division solo es \n",
    "        para fines demostrativos,en codigo final todos los metodos deben \n",
    "        pertenecer a una sola clase.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se carga el df \n",
    "            self.revisor_data = revisor_data\n",
    "            self.df = self.revisor_data.df\n",
    "            self.matriz_correlacion = None\n",
    "            self.transformaciones = {}\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    #Este metodo esta repetido en la clase graficos, al final se debe mejorar el codigo ya sea uniendo todas las clases o usando herencias.\n",
    "    def varibles_numericas(self):\n",
    "        \"\"\"Devuelve una lista con las variables numericas de un df\"\"\"\n",
    "        return [column_name for column_name, data_type in zip(self.df.columns, self.df.dtypes) if ((data_type != 'category') and  np.issubdtype(data_type, np.number))]\n",
    "    \n",
    "    def detecion_outlier(self,nombre_columna,q=.1):\n",
    "        \"\"\" \n",
    "        Funcion para detectar valores atípicos (utiliers) de una columna especifica usando cuantiles.\n",
    "        Por defecto se usa Deciles / dividiendo la distribucion en 10 partes, pero ajsutando el valor de\n",
    "          q = 0.25 se trabajaria con cuartiles.\n",
    "        La funcion devuelve una \"lista/pandas.core.indexes.numeric.Int64Index\"  con los indices de los \n",
    "        valores atípicos del df\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #calculo de cuantiles\n",
    "        Q1 = self.df[nombre_columna].quantile(q)\n",
    "        Q3 = self.df[nombre_columna].quantile(1-q)\n",
    "        IQR = Q3-Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        indice_filas_eliminar = self.df.index[(self.df[nombre_columna] < limite_inferior) | (self.df[nombre_columna] > limite_superior) ]\n",
    "        return indice_filas_eliminar   \n",
    "\n",
    "    def eliminacion_outlier(self,nombre_columna,q=0.1):\n",
    "        \"\"\" \n",
    "        Funcion para eliminar valores atípicos (outliers) de una columna especifica usando cuantiles.\n",
    "        La funcion no devuelve nada porque los cambios se hacen en el df que se pasa por referencia\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                indices = self.detecion_outlier(nombre_columna,q)\n",
    "                self.df = self.df.drop(indices)\n",
    "                self.df.reset_index(inplace=True,drop=True)\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def guardar_transformador(self,nombre_columna,transformador):\n",
    "        \"\"\"Metodo para guardar transformador aplicado, se almacenan en lista, si se\n",
    "        aplican dos o mas se almacenaran en la secuencia aplicada\"\"\"\n",
    "        if nombre_columna in list(self.transformaciones.keys()):\n",
    "            self.transformaciones[nombre_columna] = self.transformaciones[nombre_columna]+[transformador]\n",
    "        else:\n",
    "            self.transformaciones[nombre_columna] = [transformador]\n",
    "            \n",
    "    def Transf_MinMaxScaler(self,nombre_columna):\n",
    "        \"\"\"crea objeto que Transforma los valores de la columna indicada \n",
    "        usando MinMaxScaler de sklear, devuelve el objeto para transformar futuros valores\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing.MinMaxScaler()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  QuantileTransformer de sklearn\"\"\"\n",
    "        try:\n",
    "            if nombre_columna in self.varibles_numericas():\n",
    "                scaler = preprocessing. QuantileTransformer()\n",
    "                escalador = scaler.fit(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "                return escalador\n",
    "            else:\n",
    "                print(\"La variable no es numerica\")\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)   \n",
    "\n",
    "    def Transf_OneHot_binario(self, nombre_columna):\n",
    "        \"\"\"Crea objeto para transformacion  OneHot cuando la varible es binaria, devuelve\n",
    "        el transformador ya entrenado\"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_OneHot(self, nombre_columna):\n",
    "        \"\"\"crea objeto para Transformar los valores de la columna indicada  \n",
    "        usando  OneHot encoder de sklearn, devuelve el transformador entrenado \"\"\"\n",
    "        try:\n",
    "            value_var = self.df[nombre_columna].astype(\"category\")\n",
    "            codificador_oneHot = OneHotEncoder(handle_unknown='ignore')\n",
    "            codificacion = codificador_oneHot.fit(pd.DataFrame(value_var, columns=[nombre_columna]))\n",
    "            return codificacion\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def Transf_woe(self,nombre_columna,nombre_target):\n",
    "        \"\"\"crea el objeto para Transformar los valores de la columna indicada  \n",
    "        usando woe de OptimalBinning,\n",
    "        Distinge de variables numericas y categoricas\n",
    "        Devuelve el objeto transoformador\"\"\"\n",
    "        try:\n",
    "            data_type = self.df[nombre_columna].dtypes\n",
    "            target = self.df[nombre_target]\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            if (data_type == 'object' or data_type.name == 'category'):  \n",
    "                optb = OptimalBinning(name = nombre_columna,dtype ='categorical',solver='mip')\n",
    "                optb.fit(x,target)\n",
    "            elif np.issubdtype(data_type, np.number):\n",
    "                optb = OptimalBinning(name = nombre_columna,dtype = 'numerical',solver='cp')\n",
    "                optb.fit(x,target)\n",
    "            else:\n",
    "                print(\"La variable se de convertir a tipo numerica o categorica\")\n",
    "                optb = None\n",
    "                \n",
    "            return optb\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)        \n",
    "\n",
    "    def calc_matriz_correlacion(self, columnas=None,filas=None):\n",
    "        \"\"\"metodo para la creacion de la matriz de correlacion, se pude ajustar el numero de filas y las columnas\n",
    "        a calcular para la correlacion\"\"\"\n",
    "        try:\n",
    "            if self.matriz_correlacion is None or self.matriz_correlacion.empty:\n",
    "                if columnas:\n",
    "                    # Se confirma que las columnas existan y sean numericas\n",
    "                    columnas = [ col for col in columnas if col in self.varibles_numericas()]\n",
    "                else:\n",
    "                    columnas = self.varibles_numericas()\n",
    "                if filas:\n",
    "                    data_set = self.df[columnas].sample(filas)\n",
    "                    data_set.reset_index(inplace=True,drop=True)\n",
    "                else:\n",
    "                    data_set = self.df[columnas]\n",
    "                self.matriz_correlacion = data_set.corr(method='pearson', numeric_only=True)\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)    \n",
    "\n",
    "    def clasificacion_correlacion(self, target_name ,nivel_correlacion_target = 0.3, nivel_correlacion_inter_variables = 0.3):\n",
    "        \"\"\"Metodo para clasificar las variables en funcion del nivel de correlacion \n",
    "        1. Entre las variables y el objetivo (target_name), escogiendo las que tengan una \n",
    "        Moderada correlación |corr| > 0.3 pero se puede ajustar si se desea.\n",
    "        2. Entre las mismas variables permitiendo hasta una Moderada correlación |corr| < 0.3  entre ellas\n",
    "        con el objetivo de reducir la multicolinalidad\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Calculo de la correlacion para todas las variables\n",
    "            self.calc_matriz_correlacion()\n",
    "            matriz = self.matriz_correlacion\n",
    "\n",
    "            #Verificacion de la correlacion entre todas las variables y el target, ordenadas de mayor a menor\n",
    "            signals = []\n",
    "            for i in range(len(matriz)):\n",
    "                for j in range(i+1, len(matriz)):\n",
    "                    signal_1 = matriz.columns[i]\n",
    "                    signal_2 = matriz.columns[j]\n",
    "                    correlation = abs(matriz.iloc[i, j])\n",
    "                    signals.append([signal_1, signal_2, correlation])\n",
    "            df_correlacion = pd.DataFrame(signals, columns=['sig_1', 'sig_2', 'nivel_correlacion'])\n",
    "            df_correlacion_mod = df_correlacion[df_correlacion['sig_2'] == target_name].sort_values(by='nivel_correlacion', ascending=False)\n",
    "            variables_x = list(df_correlacion_mod [df_correlacion_mod['nivel_correlacion']>nivel_correlacion_target]['sig_1'])\n",
    "\n",
    "            # Verificacion de la correlacion entre variables \n",
    "            for i, variable1 in enumerate(variables_x):\n",
    "                for variable2 in variables_x[i+1:]:\n",
    "                    mask = (df_correlacion['sig_1'] == variable1) & (df_correlacion['sig_2'] == variable2)\n",
    "                    val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    if not val_corr:\n",
    "                        mask = (df_correlacion['sig_1'] == variable2) & (df_correlacion['sig_2'] == variable1)\n",
    "                        val_corr = list(df_correlacion[mask]['nivel_correlacion'])\n",
    "                    val_corr = val_corr[0] if val_corr else 0\n",
    "                    if val_corr > nivel_correlacion_inter_variables:\n",
    "                        if variable2 in variables_x:\n",
    "                            variables_x.remove(variable2)\n",
    "            return variables_x\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def crea_EDAD(self,nombre_columna = 'EDAD'):\n",
    "        \"\"\"Para crear una variable EDAD  se usa como referencia la fecha actual,\n",
    "        independiente del momento en que se corra el programa, se calcula la diferencia entre\n",
    "        la fecha de nacimiento provista y la fecha actual\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fecha_actual = datetime.datetime.now()\n",
    "            self.df[nombre_columna] = (fecha_actual - self.df['FECHANACIMIENTO']).dt.days // 365\n",
    "            self.df.drop(columns=['ANIO', 'MES','FECHANACIMIENTO'], inplace=True)\n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "            \n",
    "    def crea_EDAD_NORM(self,nombre_columna = 'EDAD'):\n",
    "        \"\"\"Para crear una variable EDAD normalizada se usa como referencia la fecha actual,\n",
    "        independiente del momento en que se corra el programa, se calcula la diferencia entre\n",
    "        la fecha de nacimiento provista y la fecha actual, se aplica una eliminacion de outliers\n",
    "        y una transformacion estandar normalizando entre el valor minimo y maximo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fecha_actual = datetime.datetime.now()\n",
    "            self.df[nombre_columna] = (fecha_actual - self.df['FECHANACIMIENTO']).dt.days // 365\n",
    "            self.eliminacion_outlier(nombre_columna)\n",
    "            escalador = self.Transf_MinMaxScaler(nombre_columna) \n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1)) \n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            self.df.drop(columns=['ANIO', 'MES','FECHANACIMIENTO'], inplace=True)\n",
    "        \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_Transf_Quantile(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando el Transf_Quantile, \n",
    "        alamcena el nombre de la columna modificada y el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_Quantile(nombre_columna)\n",
    "            self.df[nombre_columna] = escalador.transform(self.df[nombre_columna].values.reshape(-1, 1))\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "            # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot_binario(self,nombre_columna):\n",
    "        \"\"\"Modifica la columna indicada utilizando OneHot_binario, \n",
    "        almacena el nombre de la columna modificada y \n",
    "        el objeto utilizado para la transformacion\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot_binario(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            self.df[nombre_columna] = arreglo.astype(int)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "    \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "\n",
    "    def mod_columna_OneHot(self,nombre_columna):\n",
    "        try:\n",
    "            escalador = self.Transf_OneHot(nombre_columna)\n",
    "            arreglo = escalador.transform(preproceso.df[[nombre_columna]]).toarray()\n",
    "            columnas_codificadas = escalador.get_feature_names_out([nombre_columna])\n",
    "            df_codificado = pd.DataFrame(arreglo, columns=columnas_codificadas)\n",
    "\n",
    "            #Se introducen en df los valores codificados\n",
    "            pos = self.df.columns.get_loc(nombre_columna)\n",
    "            for col in df_codificado.columns:\n",
    "                self.df.insert(pos, col, df_codificado[col])\n",
    "                pos += 1\n",
    "            self.df.drop(columns=[nombre_columna], inplace=True)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "            \n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)\n",
    "        \n",
    "    def mod_woe(self,nombre_columna,nombre_target,metrica = \"woe\"):\n",
    "        \"\"\"Metodo para transformar los elementos de una columna \n",
    "        usando woe de OptimalBinning, se pueden utilizar otras metricas \n",
    "        como: \"event_rate\", \"woe\", \"indices\" and \"bins\" .\"\"\"\n",
    "        try:\n",
    "            escalador = self.Transf_woe(nombre_columna,nombre_target)\n",
    "            x = self.df.loc[:,nombre_columna]\n",
    "            self.df[nombre_columna] = escalador.transform(x, metric=metrica)\n",
    "            self.guardar_transformador(nombre_columna,escalador)\n",
    "\n",
    "             # Se actualiza la instancia de la clase revisor_data_csv\n",
    "            #self.revisor_data.df = self.df\n",
    "        except Exception as error:\n",
    "            print(\"Ocurrió un error:\", error)  \n",
    "preproceso = preprocesamiento(revision)\n",
    "def grafica_confusion_matrix(y_test, y_test_predictions):\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_predictions)\n",
    "    fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "    display = ConfusionMatrixDisplay(conf_matrix)\n",
    "    ax.set(title='Confusion Matrix')\n",
    "    display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a0c56b-82a4-4040-863e-66212a504d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9266614, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproceso = preprocesamiento(revision)\n",
    "preproceso.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f649c5-4faa-469d-a0b2-3697284b427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproceso.crea_EDAD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3ed5d5-930f-4a43-a277-69aa1434b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ESTADO', 'INGRESO', 'MORAS', 'SEXO', 'ESTADOCIVIL', 'MARCACIONES',\n",
       "       'CONTACTOS', 'M1', 'C1', 'M2', 'C2', 'M3', 'C3', 'ANTIGUEDAD', 'EDAD',\n",
       "       'TARGET'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproceso.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675d6559-fcef-4b1b-b824-04fadb47c157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ESTADO</th>\n",
       "      <th>INGRESO</th>\n",
       "      <th>MORAS</th>\n",
       "      <th>SEXO</th>\n",
       "      <th>ESTADOCIVIL</th>\n",
       "      <th>MARCACIONES</th>\n",
       "      <th>CONTACTOS</th>\n",
       "      <th>M1</th>\n",
       "      <th>C1</th>\n",
       "      <th>M2</th>\n",
       "      <th>C2</th>\n",
       "      <th>M3</th>\n",
       "      <th>C3</th>\n",
       "      <th>ANTIGUEDAD</th>\n",
       "      <th>EDAD</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9079213</th>\n",
       "      <td>ESTADO DE MEXICO</td>\n",
       "      <td>15000.00000</td>\n",
       "      <td>7</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880608</th>\n",
       "      <td>VERACRUZ</td>\n",
       "      <td>5330.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8944710</th>\n",
       "      <td>BAJA CALIFORNIA NORTE</td>\n",
       "      <td>12000.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384634</th>\n",
       "      <td>QUERETARO</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>7</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565017</th>\n",
       "      <td>CHIAPAS</td>\n",
       "      <td>6745.19774</td>\n",
       "      <td>8</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977937</th>\n",
       "      <td>TLAXCALA</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800141</th>\n",
       "      <td>PUEBLA</td>\n",
       "      <td>8000.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236095</th>\n",
       "      <td>BAJA CALIFORNIA NORTE</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772341</th>\n",
       "      <td>BAJA CALIFORNIA SUR</td>\n",
       "      <td>15000.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424741</th>\n",
       "      <td>DURANGO</td>\n",
       "      <td>7500.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40121</th>\n",
       "      <td>OAXACA</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7129734</th>\n",
       "      <td>SINALOA</td>\n",
       "      <td>8000.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413930</th>\n",
       "      <td>JALISCO</td>\n",
       "      <td>8500.00000</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>16</td>\n",
       "      <td>225</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370036</th>\n",
       "      <td>NUEVO LEON</td>\n",
       "      <td>6000.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665640</th>\n",
       "      <td>TAMAULIPAS</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ESTADO      INGRESO  MORAS SEXO ESTADOCIVIL  \\\n",
       "9079213       ESTADO DE MEXICO  15000.00000      7    M           S   \n",
       "2880608               VERACRUZ   5330.00000      2    M           S   \n",
       "8944710  BAJA CALIFORNIA NORTE  12000.00000      2    F           U   \n",
       "4384634              QUERETARO  30000.00000      7    M           C   \n",
       "1565017                CHIAPAS   6745.19774      8    F           U   \n",
       "4977937               TLAXCALA   5000.00000      2    M           S   \n",
       "3800141                 PUEBLA   8000.00000      1    F           C   \n",
       "1236095  BAJA CALIFORNIA NORTE  30000.00000      1    F           C   \n",
       "5772341    BAJA CALIFORNIA SUR  15000.00000      2    M           C   \n",
       "3424741                DURANGO   7500.00000      2    M           S   \n",
       "40121                   OAXACA   5000.00000      5    F           C   \n",
       "7129734                SINALOA   8000.00000      2    M           S   \n",
       "4413930                JALISCO   8500.00000      5    F           S   \n",
       "6370036             NUEVO LEON   6000.00000      1    F           C   \n",
       "665640              TAMAULIPAS      0.00000      1    M           C   \n",
       "\n",
       "         MARCACIONES  CONTACTOS   M1  C1   M2  C2   M3  C3  ANTIGUEDAD  EDAD  \\\n",
       "9079213            3          1    0   0    0   0    0   0           1    61   \n",
       "2880608           53          1   12   8    7   5    0   0           0    23   \n",
       "8944710           28          0    2   0    0   0    0   0           1    40   \n",
       "4384634           37          0   22   0   26   0  120   0           2    67   \n",
       "1565017           53          0  183   0   75   0  113   0          10    61   \n",
       "4977937          137          0    0   0    0   0    0   0           0    28   \n",
       "3800141           11          0    0   0    0   0    0   0           0    46   \n",
       "1236095            7          2    0   0    0   0    0   0           0    39   \n",
       "5772341           20          2    8   2    0   0    0   0           2    50   \n",
       "3424741           40          2    0   0    0   0    0   0           0    60   \n",
       "40121             65          0   21   0   42   0   33   0           1    26   \n",
       "7129734           91          0   18   0    0   0    0   0           3    26   \n",
       "4413930           77          1   69  16  225  19  100  46           0    33   \n",
       "6370036            5          3   29   6    1   1    0   0           0    45   \n",
       "665640             2          1    0   0    0   0   10   2           0    37   \n",
       "\n",
       "         TARGET  \n",
       "9079213       0  \n",
       "2880608       0  \n",
       "8944710       0  \n",
       "4384634       0  \n",
       "1565017       0  \n",
       "4977937       0  \n",
       "3800141       0  \n",
       "1236095       0  \n",
       "5772341       0  \n",
       "3424741       1  \n",
       "40121         0  \n",
       "7129734       0  \n",
       "4413930       0  \n",
       "6370036       1  \n",
       "665640        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproceso.df.sample(15,random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0e4376-68f7-4901-98ae-a9ab833b7a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6898694\n",
       "1    2367920\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproceso.df.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc18c5-e48d-4a3a-802c-bc9802da4cd9",
   "metadata": {},
   "source": [
    "Division de la data en n particones, \n",
    "el objetivo sera utilizar una particion para crear cada modelo, \n",
    "pero se hara una prueba final con otra particion\n",
    "y se hara una ultima prueba con la combinacion de los modelos usando votingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21084225-2e6a-41ca-af71-62fff8568cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Obtener una permutación aleatoria de los índices del DataFrame\n",
    "indices = np.random.permutation(revision.df.index)\n",
    "\n",
    "# Dividir los índices en 20 partes iguales (cada una un 5% de la data)\n",
    "particiones = np.array_split(indices,20)\n",
    "\n",
    "# Crear una lista de DataFrames para almacenar las partes divididas\n",
    "partes_df = []\n",
    "\n",
    "# Iterar sobre las particiones y crear un DataFrame para cada una\n",
    "for particion in particiones:\n",
    "    parte_df = preproceso.df.loc[particion]\n",
    "    parte_df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    \n",
    "    #partes_df.append(parte_df)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# # Imprimir los DataFrames resultantes\n",
    "# for i, parte_df in enumerate(partes_df):\n",
    "#     print(f\"Parte {i+1}:\\n{parte_df}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70176539-0092-42f5-a780-de4c08a60f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463330, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partes_df[19].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc08b40f-59bd-4f8a-94f9-ba93f29dbe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20210513_Challenge_AA_MANANA'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i = 0\n",
    "#partes_df[i].to_csv(f\"{self.file_path[:-4]}_ValoresPerdidos.csv \", index=False)\n",
    "\n",
    "dir (revision)\n",
    "\n",
    "revision.file_path.split('/')[-1].split('.')[-2]#.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b311873-a7f1-463d-9b7c-a37f80169d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = revision.file_path#.split('/')[-1].split('.')[-2]#.split('_')\n",
    "nombres = []\n",
    "for i in range(20):\n",
    "    nombres.append(name+'_'+str(i). zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65041173-0229-47da-9f5d-16096a818aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'..\\\\datos_internos/20210513_Challenge_AA_MANANA.csv_00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16f6ed75-aa35-4c17-b9b7-de3cb905d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\datos_internos']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revision.file_path.split('/')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a73ed20e-bb2f-4f75-814b-0cca40944d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\datos_internos'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(revision.file_path.split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b3e3ad8-67e9-4221-80ce-a148c4863223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\datos_internos/20210513_Challenge_AA_MANANA_00.csv '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "997890b5-1245-487b-9e53-1ee31ad8cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "path = f\"{''.join(revision.file_path.split('/')[:-1])+'/'+revision.file_path.split('/')[-1].split('.')[-2]+'_'+str(i).zfill(2)+'.csv'} \"\n",
    "partes_df[i].to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81ee20-6ae5-4640-a0f3-c088e75b79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = ['a','b','c']\n",
    "letras = ???? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f713910-69d9-48f0-9ddc-3c756319bdff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(Org_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mOrg_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m         partes_df\u001b[38;5;241m.\u001b[39mto_csv(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mparticiones_aleatorias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 14\u001b[0m, in \u001b[0;36mparticiones_aleatorias\u001b[1;34m(revision)\u001b[0m\n\u001b[0;32m     12\u001b[0m parte_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(Org_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mOrg_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mpartes_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "def particiones_aleatorias(revision):\n",
    "    # Obtener una permutación aleatoria de los índices del DataFrame\n",
    "    indices = np.random.permutation(revision.df.index)\n",
    "    \n",
    "    # Dividir los índices en 20 partes iguales (cada una un 5% de la data)\n",
    "    particiones = np.array_split(indices,20)\n",
    "\n",
    "    Org_path = revision.file_path\n",
    "    # Iterar sobre las particiones y crear un DataFrame para cada una\n",
    "    for i,particion in enumerate(particiones):\n",
    "        parte_df = preproceso.df.loc[particion]\n",
    "        parte_df.reset_index(inplace=True,drop=True)\n",
    "        path = f\"{''.join(Org_path.split('/')[:-1])+'/'+Org_path.split('/')[-1].split('.')[-2]+'_'+str(i).zfill(2)+'.csv'} \"\n",
    "        parte_df.to_csv(path, index=False)\n",
    "        \n",
    "particiones_aleatorias(revision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
